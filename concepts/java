JDK
Java Development Kit aka JDK is the core component of Java Environment and provides all the tools, executables, and binaries required to compile, debug, 
and execute a Java Program.
JDK is a platform-specific software and that’s why we have separate installers for Windows, Mac, and Unix systems.
We can say that JDK is the superset of JRE since it contains JRE with Java compiler, debugger, and core classes.

JRE
JRE is the implementation of JVM. It provides a platform to execute java programs. JRE consists of JVM, Java binaries, and other classes 
to execute any program successfully.
JRE doesn’t contain any development tools such as Java compiler, debugger, JShell, etc.
If you just want to execute a java program, you can install only JRE. You don’t need JDK because there is no development or compilation of java source code is required.

JVM
JVM is the heart of Java programming language. When we execute a Java program, JVM is responsible for converting the byte code
to the machine-specific code.
JVM is also platform-dependent and provides core java functions such as memory management, garbage collection, security, etc.
JVM is customizable and we can use java options to customize it. For example, allocating minimum and maximum memory to JVM.
JVM is called virtual because it provides an interface that does not depend on the underlying operating system and machine hardware.
This independence from hardware and the operating system makes java program write-once-run-anywhere.

Just-in-time Compiler (JIT)
JIT is part of the JVM that optimizes the process of converting byte code to machine-specific language. 
It compiles similar byte codes at the same time and reduces the overall time taken for the compilation of byte code to machine-specific language.
https://www.journaldev.com/546/difference-jdk-vs-jre-vs-jvm
https://beginnersbook.com/2013/05/jvm/

Source Code(.java)  -> Compiler(javac)  -> Byte Code(.class file)  -> JVM (specific to windows/mac - converts byte code to machine specific language)

Compiling a Java application does not result in a binary file, but in a bytecode – a portable, yet compact representation of the program – which will be converted 
to platform-specific machine code at the time of execution. Due to the high cost of translation from bytecode to machine code, JVM’s just-in-time (JIT) compiler 
only compiles frequently accessed code paths. Blocks executed only once (ex: during application startup) might be interpreted as inefficient, but are not compiled
as it wouldn’t provide relevant performance improvements.
Thereby, Java source code must be “compiled” twice before it can be executed:
1) Source code is compiled to bytecode (Main.java to Main.class).
2) The JIT compiler converts the Main.class (bytecode) file to native machine code.


JVM
Method Area: There is only one method area in a JVM which is shared among all the classes. This holds the class level information of each .class file.
Heap: Heap is a part of JVM memory where objects are allocated. JVM creates a Class object for each .class file.
Stack: Stack is a also a part of JVM memory but unlike Heap, it is used for storing temporary variables.
PC Registers: This keeps the track of which instruction has been executed and which one is going to be executed. 
              Since instructions are executed by threads, each thread has a separate PC register.
Native Method stack: A native method can access the runtime data areas of the virtual machine.

Heap is part of RAM or hard disk ?
The RAM is the physical memory of your computer. Heap memory is the (logical) memory reserved for the heap.
So, only part of the RAM is used as heap memory and heap memory doesn't have to be fully loaded into RAM (e.g. part of it may be swapped to disc by the OS).
It's a question of OS paging, aka virtual memory. You computer can take some of the data in RAM and write it to disk, temporarily.
The data paged out could be your application data, or even parts of the application itself, or simply other applications in their data, 
which are running in the backgroun.
Algorithm.Well as far as i know it depends which data OS will place temporarily in disk.It may place,
1: The data which is used just recently.
2: The data which will be not used frequently(in future)
3: The data which is used heavily.

Java Heap Space vs. Stack Memory: How Java Applications Allocate Memory
Java applications need a certain amount of RAM on a computer to run. Each time an object or variable is declared, it needs more RAM. 
Simply designating enough memory to hold every value declared and run each method would lead to a bloated application.
To keep application memory requirements lean, it is partitioned in ways that require less memory and allows the application to run more quickly.
The Java Virtual Machine (JVM) divides memory between Java Heap Space and Java Stack Memory in a way that only uses memory that’s needed.

What is Java Stack Memory?
This is the temporary memory where variable values are stored when their methods are invoked.  Stack will also use physical memory blocks.
After the method is finished, the memory containing those values is cleared to make room for new methods.
When a new method is invoked, a new block of memory will be created in the Stack. 
This new block will store the temporary values invoked by the method and references to objects stored in the Heap that are being used by the method.
Any values in this block are only accessible by the current method and will not exist once it ends.
This “last in, first out” method makes it easy to find the values needed and allows fast access to those values.

https://docs.oracle.com/cd/E13150_01/jrockit_jvm/jrockit/geninfo/diagnos/garbage_collect.html
HEAP
The Heap and the Nursery
Java objects reside in an area called the heap. The heap is created when the JVM starts up and may increase or decrease in size while the application runs. 4
When the heap becomes full, garbage is collected. During the garbage collection objects that are no longer used are cleared, thus making space for new objects.
Note that the JVM uses more memory than just the heap. For example Java methods, thread stacks and native handles are allocated in memory separate from the heap, 
as well as JVM internal data structures.
Heap size can be configured using the JVM parameters: Xms and Xmx, which define the initial and maximal size respectively.
Once the application exceeds the upper limit, a java.lang.OutOfMemoryError exception is thrown.

The heap is sometimes divided into two areas (or generations) called the nursery (or young space) and the old space. 
The nursery is a part of the heap reserved for allocation of new objects. When the nursery becomes full, garbage is collected by running a special young collection,
where all objects that have lived long enough in the nursery are promoted (moved) to the old space, thus freeing up the nursery for more object allocation.
When the old space becomes full garbage is collected there, a process called an old collection.
The reasoning behind a nursery is that most objects are temporary and short lived. A young collection is designed to be swift at finding newly allocated objects
that are still alive and moving them away from the nursery. Typically, a young collection frees a given amount of memory much faster than an old collection
or a garbage collection of a single-generational heap (a heap without a nursery).

In R27.2.0 and later releases, a part of the nursery is reserved as a keep area. The keep area contains the most recently allocated objects in the nursery and 
is not garbage collected until the next young collection. This prevents objects from being promoted just because they were allocated right before a young collection
started.

Object Allocation
During object allocation, the JRockit JVM distinguishes between small and large objects. The limit for when an object is considered large depends on the JVM version,
the heap size, the garbage collection strategy and the platform used, but is usually somewhere between 2 and 128 kB. Please see the documentation
for -XXtlaSize and -XXlargeObjectLimit for more information.
Small objects are allocated in thread local areas (TLAs). The thread local areas are free chunks reserved from the heap and given to a Java thread for exclusive use.
The thread can then allocate objects in its TLA without synchronizing with other threads. When the TLA becomes full, the thread simply requests a new TLA. 
The TLAs are reserved from the nursery if such exists, otherwise they are reserved anywhere in the heap.
Large objects that don’t fit inside a TLA are allocated directly on the heap. When a nursery is used, the large objects are allocated directly in old space. 
Allocation of large objects requires more synchronization between the Java threads, although the JRockit JVM uses a system of caches of free chunks of different
sizes to reduce the need for synchronization and improve the allocation speed.

Garbage Collection
Garbage collection is the process of freeing space in the heap or the nursery for allocation of new objects. 
This section describes the garbage collection in the JRockit JVM.

The Mark and Sweep Model
Generational Garbage Collection
Dynamic and Static Garbage Collection Modes
Compaction

The Mark and Sweep Model
The JRockit JVM uses the mark and sweep garbage collection model for performing garbage collections of the whole heap. 
A mark and sweep garbage collection consists of two phases, the mark phase and the sweep phase.
During the mark phase all objects that are reachable from Java threads, native handles and other root sources are marked as alive, as well as 
the objects that are reachable from these objects and so forth. This process identifies and marks all objects that are still used, 
and the rest can be considered garbage.
During the sweep phase the heap is traversed to find the gaps between the live objects. These gaps are recorded in a free list and are made available for new
object allocation.
The JRockit JVM uses two improved versions of the mark and sweep model. One is mostly concurrent mark and sweep and the other is parallel mark and sweep. 
You can also mix the two strategies, running for example mostly concurrent mark and parallel sweep.

Dynamic and Static Garbage Collection Modes
By default, the JRockit JVM uses a dynamic garbage collection mode that automatically selects a garbage collection strategy to use, aiming at optimizing the 
application throughput. You can also choose between two other dynamic garbage collection modes or select the garbage collection strategy statically. 
The following dynamic modes are available:

throughput, which optimizes the garbage collector for maximum application throughput. This is the default mode.
pausetime, which optimizes the garbage collector for short and even pause times.
deterministic, which optimizes the garbage collector for very short and deterministic pause times. This mode is only available as a part of Oracle JRockit Real Time.
The major static strategies are:

singlepar, which is a single-generational parallel garbage collector (same as parallel)
genpar, which is a two-generational parallel garbage collector
singlecon, which is a single-generational mostly concurrent garbage collector
gencon, which is a two-generational mostly concurrent garbage collector

Compaction
Objects that are allocated next to each other will not necessarily become unreachable (“die”) at the same time. This means that the heap may become fragmented after 
a garbage collection, so that the free spaces in the heap are many but small, making allocation of large objects hard or even impossible. Free spaces that are smaller
than the minimum thread local area (TLA) size can not be used at all, and the garbage collector discards them as dark matter until a future garbage collection frees 
enough space next to them to create a space large enough for a TLA.

To reduce fragmentation, the JRockit JVM compacts a part of the heap at every garbage collection (old collection). Compaction moves objects closer together and
further down in the heap, thus creating larger free areas near the top of the heap. The size and position of the compaction area as well as the compaction method
is selected by advanced heuristics, depending on the garbage collection mode used.
Compaction is performed at the beginning of or during the sweep phase and while all Java threads are paused.

Does Java pass by reference or pass by value?
Pass by Value: The method parameter values are copied to another variable and then the copied object is passed, that’s why it’s called pass by value.
Pass by Reference: An alias or reference to the actual parameter is passed to the method, that’s why it’s called pass by reference.

https://www.infoworld.com/article/3512039/does-java-pass-by-reference-or-pass-by-value.html

PASS BY VALUE
Object references are passed by value
All object references in Java are passed by value. This means that a copy of the value will be passed to a method. 
But the trick is that passing a copy of the value also changes the real value of the object. The reason is that Java object variables are simply references
that point to real objects in the memory heap. Therefore, even though Java passes parameters to methods by value, if the variable points to an object reference,
the real object will also be changed.

Are primitive types passed by value?
Like object types, primitive types are also passed by value.
Primitive types are allocated in the stack memory, so only the local value will be changed. In this case, there is no object reference.

Passing immutable object references
What if we did the same test with an immutable String object?
The JDK contains many immutable classes. Examples include the wrapper types Integer, Double, Float, Long, Boolean, BigDecimal, and String class.
Now if we try modfying the string in a method it wont change. That happens because a String object is immutable, which means that the fields inside the 
String are final and can’t be changed.

Passing mutable object references
Unlike String, most objects in the JDK are mutable, like the StringBuilder class. Now the value will be changed if we modify in the other method. You could expect
the same behaviour from any other mutable object in Java.

What to remember about object references
Java always passes parameter variables by value.
Object variables in Java always point to the real object in the memory heap.
A mutable object’s value can be changed when it is passed to a method.
An immutable object’s value cannot be changed, even if it is passed a new value.
“Passing by value” refers to passing a copy of the value.
“Passing by reference” refers to passing the real reference of the variable in memory.

JAVA MEMORY PROFILING

https://bitmovin.com/finding-memory-leaks-java-p1/
https://bitmovin.com/finding-memory-leaks-java-p2/

Memory Leaks
Let us finally talk about leaks, shall we? First, it is important to clarify that although Java is a garbage-collecting language, memory leaks can still occur.
The GC only ensures that unreferenced (unreachable) objects are cleaned up. Much like rectangles and squares, all unreferenced objects are unused 
(and thus safe to collect), but not all unused objects are unreferenced. A simple example is a static list within a class. Once the list is populated,
its entries will never be garbage collected because the JVM’s Garbage Collector does not know whether the list will ever be accessed again or not. 

the most common source for leaks in Java applications is the heap space and can often be traced back to simple programming errors such as:

 - Static fields or member fields of singleton objects harvesting object references
 - Unclosed streams or connections
 - Adding objects with no implementation of hashCode() and equals() to a HashSet because then the same key can be used over and over again to add entries to the HashSet.
 - Inefficient SQL queries which are frequently executed and where a large data set is read into memory

The Notorious OutOfMemoryError Exception
A memory leak in Java does not necessarily have to manifest itself in a java.lang.OutOfMemoryError exception. 
This exception is more like a symptom and a good indicator that there might be a leak somewhere. It usually occurs if there is insufficient space to allocate
an object in the Java heap but there are also other reasons (too large thread stacks, too much GC overhead etc.) why you might be seeing this exception in the logs.
The following enumeration lists all types of OOM exceptions and more information on what the individual exceptions mean can be found here.

java.lang.OutOfMemoryError: Java heap space
java.lang.OutOfMemoryError: GC Overhead limit exceeded
java.lang.OutOfMemoryError: Requested array size exceeds VM limit
java.lang.OutOfMemoryError: Metaspace
java.lang.OutOfMemoryError: request size bytes for reason. Out of swap space?
java.lang.OutOfMemoryError: Compressed class space
java.lang.OutOfMemoryError: reason stack_trace_with_native_method

Investigating Java Memory Issues
1) static code analysis must be executed and provide feedback on which pieces can be improved to prevent bugs or performance problems in the future. 
For the sake of early recognition, this analysis should happen at the moment a developer pushes a change to the code repository. 
Popular solutions for this are SonarQube and SpotBugs. 
leaks could be a result of issues within third-party integrations, where other leaks or unsafe calls through the Java Native Interface (JNI) are
made and are hard to debug after the fact. So, keep in mind that it is not necessarily your code that is leaking.
2) memory profiling
    Java applications use command-line programs such as: jcmd or jmap, which come with the Java Development Kit (JDK). Alternatively, 
    there are a handful of dedicated graphical profilers as well as GC-log analyzers available.

The very first step to analyzing an application suspected of having a memory leak is to verify your metrics. Questions to support your analysis include:
Which abstraction reports the problem?
Kubernetes Pod
Docker container
Native process

Which metric was used for memory usage?
RSS, Virtual etc.

Can the problem be verified by using another tool, script or metric?

These questions are necessary to determine the root cause of memory leaks, as the Kubernetes Pod can host multiple processes in addition to Java.
Virtual memory, however, is not necessarily a good metric to monitor, as it can be difficult to determine whether or not it’s backed by physical memory. 
Resident Set Size (RSS) is a measurement of “true” memory consumption of a process by not including swapped pages, therefore indicating physical RAM usage.
RSS should be considered for monitoring along with other relevant metrics; such as the working set, which includes a Linux page cache and is used by Kubernetes 
(and many other tools) to report memory usage.

3) GC-logging and Native Memory Tracking (NMT)
The next step in the analysis process is to enable verbose GC-logging and Native Memory Tracking (NMT) 
to gain additional data about the application in question. Both options can be configured via JVM parameters as shown below.

java -Xloggc:gclog.log \
    -XX:+PrintGCDetails \

    -XX:+PrintGCApplicationStoppedTime \

    -XX:+PrintGCApplicationConcurrentTime \

    -XX:+PrintGCDateStamps \

    -XX:+UseGCLogFileRotation \

    -XX:NumberOfGCLogFiles=5 \

    -XX:GCLogFileSize=2000k \

    -XX:NativeMemoryTracking=”summary” \

    -jar Application.jar

Once the JVM parameters are implemented, all Garbage Collector (GC) activities are added to a file called gclog.log within the application directory.
This log is automatically rotated at a file size of 2 MB and maintains a maximum of five log files before rotating once again. The GC log can be analyzed 
by hand or visualized using a service like GCeasy, which generate extensive reports based on the logs. Note that GC logging can even be enabled for production use,
since it’s impact on application performance is nearly nothing.

The second feature enabled by the listed command is called Native Memory Tracking (NMT) and is helpful to determine which memory region 
(heap, stack, code cache or reserved space for the GC) uses memory excessively. Once an application with NMT enabled has started, the following command will 
return a comprehensive overview of the reserved and committed memory. Spikes or anomalies within this output can point your investigation in the right direction.

jcmd <pid> VM.native_memory summary

4) Regardless of the outcome from previous actions, the next step should always be a dynamic profiling session of the running application. 
You can find dedicated tools such as YourKit Java Profiler & Java Flight Recorder (JFR), or an open-source program like Oracle’s VisualVM to help optimize
your profiling session. Tools like JFR and VisualVM integrate well with the most common Java Integrated Development Environments (IDE), while YourKit’s
solution provides the additional ability to profile containerized applications as seen below.

Profiling production applications can be very tedious, especially in scenarios where programs are deployed to an external cloud provider 
and encapsulated within a container. In this case, the Dockerfile has to be revised to inject a dedicated profiling agent in the container 
and a network tunnel must be established to access the agent of the containerized JVM. Depending on the application and agent, this can bring a performance
impact of 1-5%. In Bitmovin’s case, we were seeing much longer application start times, but once initialized, only a subtle impact was noticeable. 
The obvious solution to avoid this would be to execute the leaking program locally and profile in this way. 
Unfortunately, the leak might not be reproducible this way due to a lack of traffic or because the code path which is leaking memory is only executed in 
rare scenarios that only occur during production.

5) Alternatively, a heap dump can be created of a running application. A heap dump is a snapshot of all objects located in-memory at a specific point 
in time and is typically stored in a binary format using the .hprof file extension. Capturing a memory dump can be accomplished using the jmap command 
which is part of the JDK. In our experience, most profilers provide the functionality to create a heap dump during a live-profiling session.

jmap -dump,format=b,file=<file-path> <pid>
The time it takes to create a heap dump obviously depends on the amount of memory an application has allocated. Please keep in mind that during the 
whole capture process, the application will be halted and unresponsive! In addition, a heap dump triggers a full GC which is fairly expensive in terms of 
CPU time and has a short-term impact on responsiveness. Therefore, this option should be used sparingly and with caution. Still, a memory dump provides an 
in-depth view of allocations and can be interpreted using profiling tools to perform advanced root cause analysis.

Use Cases in Microservices

real-world example. Service A, is a web API that handles REST requests for creating, configuring and starting encodings. This application had the tendency 
to continuously consume more memory until maxing out at 2 GB per Kubernetes Pod. Additionally, the heap (which typically consumes most of the JVM’s memory)
only had a peak usage of around 600 MB. This microservice has been around for a long time and has a fairly complex code base. 
Service A also leverages multiple third-party dependencies and is built upon a legacy Spring Boot version, as shown in the table below.

Java Version	Spring Boot Version	JVM Parameters	Memory Usage / Pod
Java 8	1.5.x	-Xmx5536m	2,0 GB
Based on the analysis process described before, 
1) we started the application with GC logs enabled and looked at the output of the NMT summary. 
It became clear that the heap never used up its full capacity of 5 GB. Instead, peak usage was around 600 MB without the metaspace.
2) Since live profiling in our environment is not easily possible without a significant impact on application startup time (which leads to timeouts), 
we skipped this step and analyzed a production heap dump instead. This was made possible because JFR is included in the JVM, due to the  free-to-use OpenJDK-11
and does not have to be side-loaded with a dedicated agent. For analysis, a Pod with a relatively high memory consumption (1.5 GB) was selected. 

Looking at the top entries after determining the exact retained size shows that char[], java.util.HashMap and byte[] consumed a significant amount of memory.
Yet, one class is of special interest for this analysis: com.bitmovin.api.encodings.rest.helper.EncodingResolver. YourKit shows that there is only one instance
of this class and it uses almost 200MB of RAM. We analyzed this behavior further by right-clicking the list entry and choose Selected Objects,
which opens another view and displays the class’s inner structure as shown in the image below.

The field muxingCache is of type java.util.HashMap and contains 27511 entries. After reviewing the source code it was clear – the EncodingResolver
class implemented the singleton pattern and objects were continuously added to the muxingCache, which is never flushed. After discussing these findings internally, 
it turned out that in the earlier versions of the service, the EncodingResolver class was not a singleton, and the leak was occurred when the change was made. 
We found our very first memory leak!

Service B
Java Version	Spring Boot Version	JVM Parameters	Memory Usage / Pod
Java 8	2.x	-Xmx512m	6.4 GB

The first question that immediately came to mind when looking at the numbers from above was: How can a service with a heap boundary of 512 MB consume 6.4 GB of memory?
This is a pretty good indicator that the heap might not be the problem. For quality and consistency, GC logging and NMT have been enabled to verify this assumption.
Shortly after the deployment of the new application, we were able to generate a continuous growth memory usage graph for observation. 
As expected, the GC logs and NMT summary clearly showed that neither the heap (which was stable at around 300 MB), nor native memory were leaking. 
So where did the other 6.1 GB go?

Local profiling, stress testing, and a heap dump did not lead and were of no help to our analysis. Hence, it was time to get back to square one and 
rethink the strategy behind our analysis. This yields an important lesson that was mentioned earlier: verify the metrics that report the memory issue.

As it turns, we only examined at the Grafana dashboard and used the kubectl command to get the memory consumption of the Pods, 
assuming that the value reported was solely consumed by the JVM within the container. However, attachin the container and looking at the RAM utilization 
on a per-process basis, we determined that the JVM only consumed around 600 MB. Surprisingly, the remaining 5.8 GB were still not showing up amongst any statistics.

Eureka!! This is when we had a mental breakthrough and thought of caching. We believed that the page cache under Linux had something to do with the memory consumption.
As a matter of fact, Linux keeps files cached in memory to speed up read and write operations. When there is no more physical memory available and a process requires 
additional memory, Linux automatically shrinks this cache to provide the required resources. This realization left us with one final question: Which cloud files could
possibly consume up to 6 GB of cache? The answer to this is relatively straightforward: logs! When navigating to the directory within the container, 
where the application stores its logs, the disk usage utility reported a total file size of 6 GB. The following image shows what happened when we deleted the log 
files. Et voilà, the memory usage went down from 6.4 GB to around 620 MB!

We came to the conclusion that references to the logs or the files themselves were still part of the cache, which explained the sudden drop of memory usage when
the logs get deleted. To fix this issue a proper log-rotation policy must be implemented. However, log-rotation policies turned out to be more challenging than 
expected with the current Spring Boot version, as no upper limit for the archived log files can be defined. It’s important to note that archived logs can be 
deleted as soon as day one of launch, due to how Spring Boot uses underlying logging framework Logbacks.
